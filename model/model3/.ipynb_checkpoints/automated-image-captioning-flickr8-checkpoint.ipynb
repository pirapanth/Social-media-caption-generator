{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":343604,"sourceType":"datasetVersion","datasetId":145129}],"dockerImageVersionId":23025,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport os\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom keras.models import Sequential, Model\nfrom keras.utils import np_utils\nimport random\nfrom keras.preprocessing import image, sequence\nimport matplotlib.pyplot as plt","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data\nimages_dir = os.listdir(\"../input/flickr_data/Flickr_Data/\")\n\nimages_path = '../input/flickr_data/Flickr_Data/Images/'\ncaptions_path = '../input/flickr_data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'\ntrain_path = '../input/flickr_data/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt'\nval_path = '../input/flickr_data/Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt'\ntest_path = '../input/flickr_data/Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt'\n\ncaptions = open(captions_path, 'r').read().split(\"\\n\")\nx_train = open(train_path, 'r').read().split(\"\\n\")\nx_val = open(val_path, 'r').read().split(\"\\n\")\nx_test = open(test_path, 'r').read().split(\"\\n\")","metadata":{"_uuid":"b4bcb570212a1d3302c245c83c342bc59503fb66","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading captions as values and images as key in dictionary\ntokens = {}\n\nfor ix in range(len(captions)-1):\n    temp = captions[ix].split(\"#\")\n    if temp[0] in tokens:\n        tokens[temp[0]].append(temp[1][2:])\n    else:\n        tokens[temp[0]] = [temp[1][2:]]","metadata":{"_uuid":"1daa3e746dd354c8bdb5b9dbe18fbf8234c5e171","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# displaying an image and captions given to it\ntemp = captions[10].split(\"#\")\nfrom IPython.display import Image, display\nz = Image(filename=images_path+temp[0])\ndisplay(z)\n\nfor ix in range(len(tokens[temp[0]])):\n    print(tokens[temp[0]][ix])","metadata":{"_uuid":"544e582b0cdc6f1aca9d26a3304decf4ea33c0ba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating train, test and validation dataset files with header as 'image_id' and 'captions'\ntrain_dataset = open('flickr_8k_train_dataset.txt','wb')\ntrain_dataset.write(b\"image_id\\tcaptions\\n\")\n\nval_dataset = open('flickr_8k_val_dataset.txt','wb')\nval_dataset.write(b\"image_id\\tcaptions\\n\")\n\ntest_dataset = open('flickr_8k_test_dataset.txt','wb')\ntest_dataset.write(b\"image_id\\tcaptions\\n\")","metadata":{"_uuid":"a403f68bd34c57de30c1df0414908d83be5f8f3e","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Populating the above created files for train, test and validation dataset with image ids and captions for each of these images\nfor img in x_train:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        train_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        train_dataset.flush()\ntrain_dataset.close()\n\nfor img in x_test:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        test_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        test_dataset.flush()\ntest_dataset.close()\n\nfor img in x_val:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        val_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        val_dataset.flush()\nval_dataset.close()","metadata":{"_uuid":"b8950e40417acb1ea4708f85a29583bc00fc1bab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading 50 layer Residual Network Model and getting the summary of the model\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"\"\"<a href=\"http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\">ResNet50 Architecture</a>\"\"\"))\nmodel = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\nmodel.summary()\n# Note: For more details on ResNet50 architecture you can click on hyperlink given below","metadata":{"_uuid":"d90e1026c8ae80a62bc96d6e6dcfe581360d0e31","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function to process images\ndef preprocessing(img_path):\n    im = image.load_img(img_path, target_size=(224,224,3))\n    im = image.img_to_array(im)\n    im = np.expand_dims(im, axis=0)\n    return im","metadata":{"_uuid":"29cbce6fb4df495d99aa2b4a5cecd3a0cd5c27cb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = {}\nctr=0\nfor ix in x_train:\n    if ix == \"\":\n        continue\n    if ctr >= 3000:\n        break\n    ctr+=1\n    if ctr%1000==0:\n        print(ctr)\n    path = images_path + ix\n    img = preprocessing(path)\n    pred = model.predict(img).reshape(2048)\n    train_data[ix] = pred","metadata":{"_uuid":"cded3b69e4c6ae1d409261f0447f0b4a5bf472da","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['2513260012_03d33305cf.jpg'].shape","metadata":{"scrolled":true,"_uuid":"88e4ffcb875f7411047145fe892f04180d7ff1d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# opening train_encoded_images.p file and dumping it's content\nwith open( \"train_encoded_images.p\", \"wb\" ) as pickle_f:\n    pickle.dump(train_data, pickle_f )  ","metadata":{"_uuid":"6a66b099e752b47e827649aabfdacadc10396bc6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading image and its corresponding caption into a dataframe and then storing values from dataframe into 'ds'\npd_dataset = pd.read_csv(\"flickr_8k_train_dataset.txt\", delimiter='\\t')\nds = pd_dataset.values\nprint(ds.shape)","metadata":{"_uuid":"42f5acd4b13449ebce4c76db2f0558ec51638c89","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd_dataset.head()","metadata":{"_uuid":"02b4cd3a78f62bf5f224272d9100b88fe749ff61","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Storing all the captions from ds into a list\nsentences = []\nfor ix in range(ds.shape[0]):\n    sentences.append(ds[ix, 1])\n    \nprint(len(sentences))","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First 5 captions stored in sentences\nsentences[:5]","metadata":{"_uuid":"7b8614bf361eda776f6b320703222d20ab26d4f6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting each captions stored in 'sentences' and storing them in 'words' as list of list\nwords = [i.split() for i in sentences]","metadata":{"_uuid":"59d61c186da1dc0c16caf398c4a3791ebeb09fbf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a list of all unique words\nunique = []\nfor i in words:\n    unique.extend(i)\nunique = list(set(unique))\n\nprint(len(unique))\n\nvocab_size = len(unique)","metadata":{"_uuid":"19e3565c96f6a995494d5d8f30fd15bbb5b9c354","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vectorization\nword_2_indices = {val:index for index, val in enumerate(unique)}\nindices_2_word = {index:val for index, val in enumerate(unique)}","metadata":{"_uuid":"d33b0d8b93e182f64d650e8186574cc53210e2d2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_2_indices['UNK'] = 0\nword_2_indices['raining'] = 8253","metadata":{"_uuid":"151cb9679ef157e69a1ccce86ba1d16e615ee9d2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices_2_word[0] = 'UNK'\nindices_2_word[8253] = 'raining'","metadata":{"_uuid":"845f440a23b97b5f4baad958d864329820b4c3a0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(word_2_indices['<start>'])\nprint(indices_2_word[4011])\nprint(word_2_indices['<end>'])\nprint(indices_2_word[8051])","metadata":{"_uuid":"00938f733d8843863480b8c4caef969e1db8bf2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(word_2_indices.keys())\nprint(vocab_size)","metadata":{"_uuid":"e485745da8adc3ccf73a8e0d4671dd8f3be53c36","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0\n\nfor i in sentences:\n    i = i.split()\n    if len(i) > max_len:\n        max_len = len(i)\n\nprint(max_len)","metadata":{"_uuid":"6f3fdd75ac60cdce09f3628ee5797cb1088f02e5","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padded_sequences, subsequent_words = [], []\n\nfor ix in range(ds.shape[0]):\n    partial_seqs = []\n    next_words = []\n    text = ds[ix, 1].split()\n    text = [word_2_indices[i] for i in text]\n    for i in range(1, len(text)):\n        partial_seqs.append(text[:i])\n        next_words.append(text[i])\n    padded_partial_seqs = sequence.pad_sequences(partial_seqs, max_len, padding='post')\n\n    next_words_1hot = np.zeros([len(next_words), vocab_size], dtype=np.bool)\n    \n    #Vectorization\n    for i,next_word in enumerate(next_words):\n        next_words_1hot[i, next_word] = 1\n        \n    padded_sequences.append(padded_partial_seqs)\n    subsequent_words.append(next_words_1hot)\n    \npadded_sequences = np.asarray(padded_sequences)\nsubsequent_words = np.asarray(subsequent_words)\n\nprint(padded_sequences.shape)\nprint(subsequent_words.shape)","metadata":{"_uuid":"a42a7cf8e3aad67f1c750e0c8490ca211c85dbee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(padded_sequences[0])","metadata":{"_uuid":"09373074d19bd3c6784f9bb1ba832e18757e5866","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ix in range(len(padded_sequences[0])):\n    for iy in range(max_len):\n        print(indices_2_word[padded_sequences[0][ix][iy]],)\n    print(\"\\n\")\n\nprint(len(padded_sequences[0]))","metadata":{"_uuid":"d645ff6f92906aa9b54da013ef3bdfe8d0db9b11","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_of_images = 2000","metadata":{"_uuid":"e6684d2f913fe0b7a91a0157ffc5429544ded4a3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions = np.zeros([0, max_len])\nnext_words = np.zeros([0, vocab_size])","metadata":{"_uuid":"a85536f1ac94df3691b958a6ecdac7791b4f4330","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ix in range(num_of_images):#img_to_padded_seqs.shape[0]):\n    captions = np.concatenate([captions, padded_sequences[ix]])\n    next_words = np.concatenate([next_words, subsequent_words[ix]])\n\nnp.save(\"captions.npy\", captions)\nnp.save(\"next_words.npy\", next_words)\n\nprint(captions.shape)\nprint(next_words.shape)","metadata":{"_uuid":"e32cac59ecebb0bcab5a0838bec8aa7147dccdd3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/train_encoded_images.p', 'rb') as f:\n    encoded_images = pickle.load(f, encoding=\"bytes\")","metadata":{"_uuid":"02028d4bfa3e1d80b0b796f88c31f2ae4096f54e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs = []\n\nfor ix in range(ds.shape[0]):\n    if ds[ix, 0].encode() in encoded_images.keys():\n#         print(ix, encoded_images[ds[ix, 0].encode()])\n        imgs.append(list(encoded_images[ds[ix, 0].encode()]))\n\nimgs = np.asarray(imgs)\nprint(imgs.shape)","metadata":{"_uuid":"b526815565544abf9bd16d2ad0a1071947b75144","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = []\n\nfor ix in range(num_of_images):\n    for iy in range(padded_sequences[ix].shape[0]):\n        images.append(imgs[ix])\n        \nimages = np.asarray(images)\n\nnp.save(\"images.npy\", images)\n\nprint(images.shape)","metadata":{"_uuid":"cda0d325b7e402d20b254d4e5939c9b0327b8e87","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_names = []\n\nfor ix in range(num_of_images):\n    for iy in range(padded_sequences[ix].shape[0]):\n        image_names.append(ds[ix, 0])\n        \nimage_names = np.asarray(image_names)\n\nnp.save(\"image_names.npy\", image_names)\n\nprint(len(image_names))","metadata":{"_uuid":"678062934e0778d878909f3de636dc614fc9c6eb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  **Model**","metadata":{"_uuid":"20bdfb97e1fe5e0975bbddfbbd0b6591c63e2590"}},{"cell_type":"code","source":"captions = np.load(\"captions.npy\")\nnext_words = np.load(\"next_words.npy\")\n\nprint(captions.shape)\nprint(next_words.shape)","metadata":{"_uuid":"08b9b60342c0fccdda6a848b01042435420e5eda","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = np.load(\"images.npy\")\n\nprint(images.shape)","metadata":{"_uuid":"d697bdbda67ca8a545f71f195dbd6f2b32158406","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imag = np.load(\"image_names.npy\")\n        \nprint(imag.shape)","metadata":{"_uuid":"060ebdd5bec757c5a8ad7014978dbdde00b5ef04","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_size = 128\nmax_len = 40","metadata":{"_uuid":"1cd12f3c270d2e1eef4ae3943ca3ef08d6938848","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()","metadata":{"_uuid":"efc916915e0ba0cf6fdd2809d46be036c84fb139","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"language_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()","metadata":{"_uuid":"5cf54037a1ea92489312001dbd3c130ae635e940","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n# model.load_weights(\"../input/model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","metadata":{"_uuid":"dd6d7b4455ced9778ad57a3ed7a38728eea42247","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = model.fit([images, captions], next_words, batch_size=512, epochs=200)","metadata":{"scrolled":true,"_uuid":"3e249d667ba7198e22fa20de78f3d8058baa9dd3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"model_weights.h5\")","metadata":{"_uuid":"97f7316a6340f1776b5fbd3175a9d90e17d885aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predictions","metadata":{"_uuid":"2146e3152e522d8ffafd29d8df8becaa04721552"}},{"cell_type":"code","source":"def preprocessing(img_path):\n    im = image.load_img(img_path, target_size=(224,224,3))\n    im = image.img_to_array(im)\n    im = np.expand_dims(im, axis=0)\n    return im","metadata":{"_uuid":"6e67941c74369e855681b6fb16fecbd680bacebb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_encoding(model, img):\n    image = preprocessing(img)\n    pred = model.predict(image).reshape(2048)\n    return pred","metadata":{"_uuid":"1634b8e436f0db618937db0146ba2ee6e14989af","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')","metadata":{"_uuid":"af447476805eb38dfcc9ee2a2cbee283d628a0f1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = \"../input/flickr_data/Flickr_Data/Images/1453366750_6e8cf601bf.jpg\"\n\ntest_img = get_encoding(resnet, img)","metadata":{"_uuid":"90c83e00652b81ad6d352b7bda37e031148a6dba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_captions(image):\n    start_word = [\"<start>\"]\n    while True:\n        par_caps = [word_2_indices[i] for i in start_word]\n        par_caps = sequence.pad_sequences([par_caps], maxlen=max_len, padding='post')\n        preds = model.predict([np.array([image]), np.array(par_caps)])\n        word_pred = indices_2_word[np.argmax(preds[0])]\n        start_word.append(word_pred)\n        \n        if word_pred == \"<end>\" or len(start_word) > max_len:\n            break\n            \n    return ' '.join(start_word[1:-1])\n\nArgmax_Search = predict_captions(test_img)","metadata":{"_uuid":"1323e6c6436c9b71303f4429522a693dd8213a5a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = Image(filename=img)\ndisplay(z)\n\nprint(Argmax_Search)","metadata":{"_uuid":"76fe5dfea37475cacfe0c463da20cba1e5e288b5","trusted":true},"execution_count":null,"outputs":[]}]}